{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:06:23.658608100Z",
     "start_time": "2023-05-13T17:06:23.647847600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "from keras.utils import array_to_img, load_img\n",
    "from keras.layers import TextVectorization\n",
    "from keras.applications.resnet import ResNet152, preprocess_input\n",
    "from keras.utils import Progbar\n",
    "import matplotlib.pyplot as plt\n",
    "from mrcnn.utils import extract_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:06:24.687809Z",
     "start_time": "2023-05-13T17:06:24.639252600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = int(time.time())\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "CAPTIONS_PATH = \"./annotations/captions_train2014.json\"\n",
    "IMAGES_PATH = \"./train2014/train2014\"\n",
    "IMAGE_SIZE = (224, 224)\n",
    "VOCAB_SIZE = 30000\n",
    "SEQ_LENGTH=50\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:06:28.369834Z",
     "start_time": "2023-05-13T17:06:25.009242800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  66226\n",
      "Number of validation samples:  16557\n"
     ]
    }
   ],
   "source": [
    "def load_captions_data(filename):\n",
    "    \"\"\"Loads captions (text) data and maps them to corresponding images.\n",
    "\n",
    "    Args:\n",
    "        filename: Path to the text file containing caption data.\n",
    "\n",
    "    Returns:\n",
    "        caption_mapping: Dictionary mapping image names and the corresponding captions\n",
    "        text_data: List containing all the available captions\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        json_data = json.load(f)\n",
    "        imageList = json_data[\"images\"]\n",
    "        captions = json_data[\"annotations\"]\n",
    "\n",
    "        images = {}\n",
    "        # Index all images\n",
    "        for image in imageList:\n",
    "            imageId, imageName = image[\"id\"], image[\"file_name\"]\n",
    "            images[imageId] = os.path.join(IMAGES_PATH, imageName)\n",
    "\n",
    "        caption_mapping = {}\n",
    "        text_data = []\n",
    "        images_to_skip = set()\n",
    "\n",
    "        for captionDict in captions:\n",
    "            if captionDict[\"image_id\"] not in images:\n",
    "                continue\n",
    "\n",
    "            img_name = images[captionDict[\"image_id\"]]\n",
    "            caption = captionDict[\"caption\"]\n",
    "\n",
    "            # We will remove caption that are either too short to too long\n",
    "            tokens = caption.strip().split()\n",
    "\n",
    "            if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:\n",
    "                images_to_skip.add(img_name)\n",
    "                continue\n",
    "\n",
    "            if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n",
    "                # We will add a start and an end token to each caption\n",
    "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
    "                text_data.append(caption)\n",
    "\n",
    "                if img_name in caption_mapping:\n",
    "                    if len(caption_mapping[img_name]) < 5:\n",
    "                        caption_mapping[img_name].append(caption)\n",
    "                else:\n",
    "                    caption_mapping[img_name] = [caption]\n",
    "\n",
    "        for img_name in images_to_skip:\n",
    "            if img_name in caption_mapping:\n",
    "                del caption_mapping[img_name]\n",
    "\n",
    "        return caption_mapping, text_data\n",
    "\n",
    "\n",
    "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
    "    \"\"\"Split the captioning dataset into train and validation sets.\n",
    "\n",
    "    Args:\n",
    "        caption_data (dict): Dictionary containing the mapped caption data\n",
    "        train_size (float): Fraction of all the full dataset to use as training data\n",
    "        shuffle (bool): Whether to shuffle the dataset before splitting\n",
    "\n",
    "    Returns:\n",
    "        Traning and validation datasets as two separated dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Get the list of all image names\n",
    "    all_images = list(caption_data.keys())\n",
    "\n",
    "    # 2. Shuffle if necessary\n",
    "    if shuffle:\n",
    "        np.random.shuffle(all_images)\n",
    "\n",
    "    # 3. Split into training and validation sets\n",
    "    train_size = int(len(caption_data) * train_size)\n",
    "\n",
    "    training_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
    "    }\n",
    "    validation_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
    "    }\n",
    "\n",
    "    # 4. Return the splits\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "captions_mapping, text_data = load_captions_data(CAPTIONS_PATH)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, valid_data = train_val_split(captions_mapping)\n",
    "print(\"Number of training samples: \", len(train_data))\n",
    "print(\"Number of validation samples: \", len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:06:52.455685400Z",
     "start_time": "2023-05-13T17:06:28.345589100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "strip_chars = \"!\\\"#$%&'()*+,-./:;=?@[\\]^_`{|}~\"\n",
    "\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQ_LENGTH,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "vectorization.adapt(text_data)\n",
    "VOCAB_SIZE = vectorization.vocabulary_size()\n",
    "vocab = vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:06:54.125974300Z",
     "start_time": "2023-05-13T17:06:52.455685400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "\n",
    "def decode_and_resize(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_input(img_path, captions):\n",
    "    return decode_and_resize(img_path), vectorization(captions)\n",
    "\n",
    "\n",
    "def make_dataset(images, captions):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n",
    "    dataset = dataset.shuffle(BATCH_SIZE * 8)\n",
    "    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# # Pass the list of images and the list of corresponding captions\n",
    "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
    "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:06:54.206490400Z",
     "start_time": "2023-05-13T17:06:54.137245Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    base_model = ResNet152(\n",
    "        input_shape=(*IMAGE_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "    # We freeze our feature extractor\n",
    "    base_model.trainable = False\n",
    "    base_model_out = base_model.output\n",
    "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
    "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
    "    return cnn_model\n",
    "\n",
    "def get_glove_embeddings(vocab):\n",
    "    word_lookup = dict(zip(vocab, range(VOCAB_SIZE)))\n",
    "\n",
    "    glove_path = \"glove.6B/glove.6B.300d.txt\"\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            if word in word_lookup:\n",
    "                embeddings_index[word] = coefs\n",
    "\n",
    "    print(\"Found %s word vectors\" % len(embeddings_index))\n",
    "    embedding_matrix = np.zeros((VOCAB_SIZE, 300))\n",
    "    for word, i in word_lookup.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    pos = np.arange(position)[:, np.newaxis]\n",
    "    den = np.power(\n",
    "        10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2) / np.float32(d_model))\n",
    "    )\n",
    "    angle_rads = pos / den\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class SeqEmbedding(layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, seq_length, word_embeddings):\n",
    "        super().__init__()\n",
    "        self.glove_embedding = layers.Embedding(\n",
    "            vocab_size,\n",
    "            300,\n",
    "            embeddings_initializer=keras.initializers.Constant(word_embeddings),\n",
    "            mask_zero=True,\n",
    "        )\n",
    "        self.resizer = layers.Dense(d_model, activation=\"relu\")\n",
    "        self.pos_encoding = positional_encoding(seq_length, d_model)\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.glove_embedding(x)\n",
    "        x = self.resizer(x)\n",
    "        return self.add([x, self.pos_encoding[tf.newaxis, :length, :]])\n",
    "\n",
    "class BaseAttention(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "        self.add = layers.Add()\n",
    "\n",
    "\n",
    "# Decoder cross attention layer\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context, training):\n",
    "        attn_output = self.mha(query=x, key=context, value=context, training=training)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.mha(query=x, key=x, value=x, training=training)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.mha(\n",
    "            query=x, key=x, value=x, use_causal_mask=True, training=training\n",
    "        )\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dff, activation=\"relu\"),\n",
    "                layers.Dense(d_model),\n",
    "                layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.add = layers.Add()\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        x = self.self_attention(x, training)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "        self.dense = layers.Dense(d_model, activation=\"relu\")\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(\n",
    "                d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context, training):\n",
    "        x = self.causal_self_attention(x, training)\n",
    "        x = self.cross_attention(x, context, training)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        dff,\n",
    "        vocab_size,\n",
    "        seq_length,\n",
    "        word_embeddings,\n",
    "        dropout_rate=0.1\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = SeqEmbedding(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            seq_length=seq_length,\n",
    "            word_embeddings=word_embeddings,\n",
    "        )\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(\n",
    "                d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
    "\n",
    "    def call(self, x, context, training):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, context, training)\n",
    "\n",
    "        # self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "        x = self.out(x)\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:06:54.250992500Z",
     "start_time": "2023-05-13T17:06:54.218997600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DenseCaptioner(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnn_model,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        num_captions_per_image=5,\n",
    "        image_aug=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
    "        self.num_captions_per_image = num_captions_per_image\n",
    "        self.image_aug = image_aug\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = (y_true != 0) & (loss < 1e8)\n",
    "        mask = tf.cast(mask, loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred):\n",
    "        mask = y_true != 0\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
    "        encoder_out = self.encoder(img_embed, training=training)\n",
    "        batch_seq_inp = batch_seq[:, :-1]\n",
    "        batch_seq_true = batch_seq[:, 1:]\n",
    "        batch_seq_pred = self.decoder(batch_seq_inp, encoder_out, training=training)\n",
    "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred)\n",
    "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred)\n",
    "        return loss, acc\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        img_embed = self.cnn_model(preprocess_input(batch_img))\n",
    "\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, acc = self._compute_caption_loss_and_acc(\n",
    "                    img_embed, batch_seq[:, i, :], training=True\n",
    "                )\n",
    "                batch_loss += loss\n",
    "                batch_acc += acc\n",
    "\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "            )\n",
    "            grads = tape.gradient(loss, train_vars)\n",
    "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 8. Return the loss and accuracy values\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    def test_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        img_embed = self.cnn_model(preprocess_input(batch_img))\n",
    "\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            loss, acc = self._compute_caption_loss_and_acc(\n",
    "                img_embed, batch_seq[:, i, :], training=False\n",
    "            )\n",
    "\n",
    "            # 3. Update batch loss and batch accuracy\n",
    "            batch_loss += loss\n",
    "            batch_acc += acc\n",
    "\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "\n",
    "        # 4. Update the trackers\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 5. Return the loss and accuracy values\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker, self.acc_tracker]\n",
    "\n",
    "    def forward(self, img):\n",
    "        img = self.cnn_model(preprocess_input(img))\n",
    "\n",
    "        # Pass the image features to the Transformer encoder\n",
    "        encoded_img = self.encoder(img, training=False)\n",
    "\n",
    "        # Generate the caption using the Transformer decoder\n",
    "        decoded_caption = \"<start> \"\n",
    "        for i in range(max_decoded_sentence_length):\n",
    "            tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
    "            predictions = self.decoder(\n",
    "                tokenized_caption, encoded_img, training=False\n",
    "            )\n",
    "            sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "            sampled_token = index_lookup[sampled_token_index]\n",
    "            if sampled_token == \"<end>\":\n",
    "                break\n",
    "            decoded_caption += \" \" + sampled_token\n",
    "\n",
    "        decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
    "        decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
    "        return decoded_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:07:30.132567600Z",
     "start_time": "2023-05-13T17:06:54.239582300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19795 word vectors\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 512\n",
    "FF_DIM = 512\n",
    "EPOCHS = 10\n",
    "NUM_HEADS = 8\n",
    "ENCODER_LAYERS = 2\n",
    "DECODER_LAYERS = 2\n",
    "\n",
    "cnn_model = get_cnn_model()\n",
    "word_embeddings = get_glove_embeddings(vocab)\n",
    "\n",
    "# Init transformer blocks\n",
    "encoder = Encoder(\n",
    "    num_layers=ENCODER_LAYERS, d_model=EMBED_DIM, num_heads=NUM_HEADS, dff=FF_DIM\n",
    ")\n",
    "decoder = Decoder(\n",
    "    num_layers=DECODER_LAYERS,\n",
    "    d_model=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    dff=FF_DIM,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    word_embeddings=word_embeddings,\n",
    ")\n",
    "\n",
    "# Init dense captioner\n",
    "caption_model = DenseCaptioner(\n",
    "    cnn_model=cnn_model,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:07:32.802396600Z",
     "start_time": "2023-05-13T17:07:30.132567600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"none\"\n",
    ")\n",
    "\n",
    "# EarlyStopping criteria\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler for the optimizer\n",
    "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        global_step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        warmup_progress = global_step / warmup_steps\n",
    "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
    "        return tf.cond(\n",
    "            global_step < warmup_steps,\n",
    "            lambda: warmup_learning_rate,\n",
    "            lambda: self.post_warmup_learning_rate,\n",
    "        )\n",
    "\n",
    "\n",
    "# Create a learning rate schedule\n",
    "num_train_steps = len(train_dataset) * EPOCHS\n",
    "num_warmup_steps = num_train_steps // 15\n",
    "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n",
    "\n",
    "# Compile the model\n",
    "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)\n",
    "\n",
    "checkpoint_path = \"training-glove/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    caption_model.load_weights(latest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   1/1035 [..............................] - ETA: 18:24:11 - loss: 9.2075 - acc: 0.5590"
     ]
    }
   ],
   "source": [
    "caption_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[early_stopping, cp_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    \"BG\",\n",
    "    \"person\",\n",
    "    \"bicycle\",\n",
    "    \"car\",\n",
    "    \"motorcycle\",\n",
    "    \"airplane\",\n",
    "    \"bus\",\n",
    "    \"train\",\n",
    "    \"truck\",\n",
    "    \"boat\",\n",
    "    \"traffic light\",\n",
    "    \"fire hydrant\",\n",
    "    \"stop sign\",\n",
    "    \"parking meter\",\n",
    "    \"bench\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"sheep\",\n",
    "    \"cow\",\n",
    "    \"elephant\",\n",
    "    \"bear\",\n",
    "    \"zebra\",\n",
    "    \"giraffe\",\n",
    "    \"backpack\",\n",
    "    \"umbrella\",\n",
    "    \"handbag\",\n",
    "    \"tie\",\n",
    "    \"suitcase\",\n",
    "    \"frisbee\",\n",
    "    \"skis\",\n",
    "    \"snowboard\",\n",
    "    \"sports ball\",\n",
    "    \"kite\",\n",
    "    \"baseball bat\",\n",
    "    \"baseball glove\",\n",
    "    \"skateboard\",\n",
    "    \"surfboard\",\n",
    "    \"tennis racket\",\n",
    "    \"bottle\",\n",
    "    \"wine glass\",\n",
    "    \"cup\",\n",
    "    \"fork\",\n",
    "    \"knife\",\n",
    "    \"spoon\",\n",
    "    \"bowl\",\n",
    "    \"banana\",\n",
    "    \"apple\",\n",
    "    \"sandwich\",\n",
    "    \"orange\",\n",
    "    \"broccoli\",\n",
    "    \"carrot\",\n",
    "    \"hot dog\",\n",
    "    \"pizza\",\n",
    "    \"donut\",\n",
    "    \"cake\",\n",
    "    \"chair\",\n",
    "    \"couch\",\n",
    "    \"potted plant\",\n",
    "    \"bed\",\n",
    "    \"dining table\",\n",
    "    \"toilet\",\n",
    "    \"tv\",\n",
    "    \"laptop\",\n",
    "    \"mouse\",\n",
    "    \"remote\",\n",
    "    \"keyboard\",\n",
    "    \"cell phone\",\n",
    "    \"microwave\",\n",
    "    \"oven\",\n",
    "    \"toaster\",\n",
    "    \"sink\",\n",
    "    \"refrigerator\",\n",
    "    \"book\",\n",
    "    \"clock\",\n",
    "    \"vase\",\n",
    "    \"scissors\",\n",
    "    \"teddy bear\",\n",
    "    \"hair drier\",\n",
    "    \"toothbrush\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:11:16.095912800Z",
     "start_time": "2023-05-13T17:11:16.064033600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
    "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
    "valid_images = list(valid_data.keys())\n",
    "\n",
    "\n",
    "def generate_caption(img_id):\n",
    "    sample_img = decode_and_resize(img_id)\n",
    "    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n",
    "    img = tf.expand_dims(sample_img, 0)\n",
    "    decoded_caption = caption_model.forward(img)\n",
    "    return decoded_caption\n",
    "\n",
    "def generate_dense_caption(img_path, rcnn):\n",
    "    # sample_img = np.random.choice(valid_images)\n",
    "    sample_img = img_path\n",
    "\n",
    "    Img = np.array(load_img(sample_img))\n",
    "    results = rcnn.detect([Img], verbose=0)\n",
    "    masks = results[0][\"masks\"][:, :, :8]\n",
    "    # Rois = extract_bboxes(masks)\n",
    "    # masks = np.transpose(masks, (2, 0, 1))\n",
    "    # num_rois = masks.shape[0]\n",
    "    rois = results[0]['rois']\n",
    "    Rois = rois[:8]\n",
    "    # print(rois)\n",
    "    classes = results[0]['class_ids']\n",
    "    scores = results[0]['scores']\n",
    "    regions = []\n",
    "    decoded_captions = []\n",
    "    for j in range(len(Rois)):\n",
    "        start1 = max(0, Rois[j][0] - 40)\n",
    "        start2 = max(0, Rois[j][1] - 40)\n",
    "        height = min(Img.shape[0] - start1, (Rois[j][2] - Rois[j][0]) * 2)\n",
    "        wd = min(Img.shape[1] - start2, (Rois[j][3] - Rois[j][1]) * 2)\n",
    "        masked_roi = tf.image.crop_to_bounding_box(\n",
    "            Img,\n",
    "            start1,\n",
    "            start2,\n",
    "            height,\n",
    "            wd,\n",
    "        )\n",
    "        class_name = CLASS_NAMES[ classes[j] ]\n",
    "        regions.append([class_name, masked_roi])\n",
    "        masked_roi = tf.image.resize(masked_roi, IMAGE_SIZE, 'lanczos5', antialias=True)\n",
    "        masked_roi = tf.expand_dims(masked_roi,0)\n",
    "        decoded_captions.append(caption_model.forward(masked_roi))\n",
    "    return decoded_captions, scores, regions, masks, classes[:8], Rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:10:47.550474900Z",
     "start_time": "2023-05-13T17:10:43.044920100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\aad3s\\Desktop\\FYP\\FYPII\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From c:\\Users\\aad3s\\Desktop\\FYP\\FYPII\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "WARNING:tensorflow:From c:\\Users\\aad3s\\Desktop\\FYP\\FYPII\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "class MyConfig(Config):\n",
    "    NAME = \"my_config\"\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 80  # COCO dataset has 80 classes + background\n",
    "\n",
    "\n",
    "model_rcnn = MaskRCNN(mode=\"inference\", model_dir=os.getcwd(), config=MyConfig())\n",
    "model_rcnn.load_weights(\"mask_rcnn_coco.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = \"test6.jpg\"\n",
    "# plt.imshow(load_img(img_id))\n",
    "# plt.show()\n",
    "# captions = generate_caption(img_id)\n",
    "captions, scores, regions = generate_dense_caption(img_id, model_rcnn)\n",
    "# print(scores)\n",
    "# print(captions)\n",
    "for caption in captions:\n",
    "    print(caption)\n",
    "for i in range(len(regions)):\n",
    "    print(scores[i])\n",
    "\n",
    "for k in regions:\n",
    "    print(k[0])\n",
    "    plt.imshow(k[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flask import Flask, request\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from flask import jsonify\n",
    "from flask_cors import CORS\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pybase64\n",
    "\n",
    "model = None\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "def load_model():\n",
    "    global model\n",
    "    model = keras.models.load_model('AlexNet_Model_Local_Dataset_Trained.h5')\n",
    "\n",
    "@app.route('/')\n",
    "def home_endpoint():\n",
    "    return 'Hello World!'\n",
    "\n",
    "def image_array_to_base64(image_array):\n",
    "    # Convert image array to image object\n",
    "    image = np.array(image_array, dtype=np.uint8)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    _, encoded_image = cv2.imencode(\".png\", image)\n",
    "\n",
    "    # Convert image to Base64 string\n",
    "    base64_string = pybase64.b64encode(encoded_image.tobytes()).decode(\"utf-8\")\n",
    "\n",
    "    return base64_string\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def get_prediction():\n",
    "    if request.method == 'POST':\n",
    "\n",
    "        if 'image' not in request.files:\n",
    "            return jsonify({'error': 'No file part'})\n",
    "        file = request.files['image']\n",
    "        img = Image.open(file.stream)\n",
    "        img = img.resize((256, 256))\n",
    "        img_arr = np.array(img)\n",
    "        img_arr = np.expand_dims(img_arr, axis=0)\n",
    "        img_arr = img_arr/255.0\n",
    "\n",
    "\n",
    "        prediction = model.predict(img_arr)  \n",
    "        d = prediction.flatten()\n",
    "        print(d)\n",
    "        j = d.max()\n",
    "        print(j)\n",
    "        for index,item in enumerate(d):\n",
    "            if item == j:\n",
    "                class_name = li[index]\n",
    "    return jsonify({'prediction': str(class_name)})\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    app.run(host='0.0.0.0', port=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:07:37.656779600Z",
     "start_time": "2023-05-13T17:07:37.641058400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Targets = list(valid_data.items())[:16]\n",
    "# Targets = [\"kitchen.jpg\", \"kitchen2.jpg\"]\n",
    "from mrcnn import visualize\n",
    "for idx, (img_id, captions) in enumerate(Targets):\n",
    "    captions, scores, regions, masks, classes, Rois = generate_dense_caption(img_id, model_rcnn)\n",
    "    plt.imshow(load_img(img_id))\n",
    "    plt.show()\n",
    "    print(img_id)\n",
    "    visualize.display_instances(\n",
    "        image=np.array(load_img(img_id)),\n",
    "        boxes=np.array(Rois),\n",
    "        masks=np.array(masks),\n",
    "        class_ids=np.array(classes),\n",
    "        class_names=CLASS_NAMES,\n",
    "        scores=np.array(scores[:8]),\n",
    "    )\n",
    "    print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:19:12.083988900Z",
     "start_time": "2023-05-13T17:16:32.674105Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Targets = list(valid_data.items())[:1000]\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "print(len(Targets))\n",
    "for idx, (img_id, captions) in enumerate(Targets):\n",
    "    predictions = generate_dense_caption(img_id, model_rcnn)\n",
    "    print(idx)\n",
    "    prediction = generate_caption(img_id)\n",
    "    ref = [''] * 5\n",
    "    for i in range(len(captions)):\n",
    "        ref[i] = captions[i].split()\n",
    "        ref[i] = ref[i][1 : len(ref[i]) - 1]\n",
    "    references.append(ref)\n",
    "    predictions.append(prediction.split())\n",
    "    \n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35216819889450796\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge \n",
    "rouge = Rouge()\n",
    "\n",
    "scores = 0\n",
    "cnt = 0\n",
    "for i in range(len(predictions)):\n",
    "    prediction = ' '.join(predictions[i])\n",
    "    for j in range(5):\n",
    "        ref=' '.join(references[i][j])\n",
    "        score = rouge.get_scores(prediction, ref)\n",
    "        scores += score[0]['rouge-l']['f']\n",
    "        cnt += 1\n",
    "        \n",
    "print(scores / cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preds_glove.txt', 'w') as f:\n",
    "    for p in predictions:\n",
    "        p = ' '.join(p)\n",
    "        # print(p)\n",
    "        f.write(p + '\\n')\n",
    "\n",
    "with open('refs_glove.txt', 'w') as f:\n",
    "    for ref in references:\n",
    "        for r in ref:\n",
    "            r = ' '.join(r)\n",
    "            print(r)\n",
    "            f.write(r + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
